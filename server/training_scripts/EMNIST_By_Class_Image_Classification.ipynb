{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EMNIST By Class - Image Classification",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBzpxy7iVoQe",
        "colab_type": "text"
      },
      "source": [
        "## Image Classification with EMNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "grRIjAabcqTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z7AFehVUMYh",
        "colab_type": "text"
      },
      "source": [
        "### Obtaining the EMNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thYLjKf5voi-",
        "colab_type": "code",
        "outputId": "7138d96c-e5f2-4bd0-93e4-7a76b301f3c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "source": [
        "pip install emnist"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting emnist\n",
            "  Downloading https://files.pythonhosted.org/packages/d1/f4/78b24acbef9e8fe976dda700f16a3606f3b8363b015bc555f8050fbbd8ac/emnist-0.0-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from emnist) (2.21.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from emnist) (1.16.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from emnist) (4.28.1)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (2019.3.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->emnist) (1.24.3)\n",
            "Installing collected packages: emnist\n",
            "Successfully installed emnist-0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTEIbr4BIiSa",
        "colab_type": "code",
        "outputId": "c5d1b05e-3323-4c35-8fee-7354c9a096de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# Only use this if using iPython\n",
        "%matplotlib inline\n",
        "\n",
        "from emnist import extract_training_samples, extract_test_samples\n",
        "x_train, y_train = extract_training_samples('byclass')\n",
        "x_test, y_test   = extract_test_samples('byclass')\n",
        "\n",
        "print(x_train.shape, x_test.shape)\n",
        "\n",
        "i = 0\n",
        "print(\"LABELED AS:\", y_train[i])\n",
        "plt.imshow(x_train[i])\n",
        "\n",
        "#print(x_train[0])\n",
        "image = Image.open(\"pic1.png\")\n",
        "print(image.show())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(697932, 28, 28) (116323, 28, 28)\n",
            "LABELED AS: 35\n",
            "None\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEvV/opcbXuwFGEkAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gLW3Kn8UafA",
        "colab_type": "text"
      },
      "source": [
        "### Reshaping the training and test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2HtE2HoSEWQ",
        "colab_type": "code",
        "outputId": "f47c3016-43e2-4d7d-f606-df32425e179e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# NEW\n",
        "x_train = x_train.reshape(697932, 28, 28)\n",
        "y_train = y_train.reshape(697932, 1)\n",
        "x_test = x_test.reshape(116323, 28, 28)\n",
        "y_test = y_test.reshape(116323, 1)\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1).astype('float32')/255\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1).astype('float32')/255\n",
        "input_shape = (28, 28, 1)\n",
        "num_classes = 62\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (697932, 28, 28, 1)\n",
            "Number of images in x_train 697932\n",
            "Number of images in x_test 116323\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86CVgm4YUhmz",
        "colab_type": "text"
      },
      "source": [
        "### Making, training and evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFDAPnRESwuW",
        "colab_type": "code",
        "outputId": "8c561435-e033-4b96-efac-66fd94fa8a8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "# Importing the required Keras modules containing model and layers\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "# Creating a Sequential Model and adding the layers\n",
        "# model = Sequential()\n",
        "# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "# model.add(Dropout(0.25))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(128, activation='relu'))\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(num_classes, activation='softmax'))\n",
        "# Loss 0.\n",
        "\n",
        "# Optimized model\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), padding='valid', activation='relu', input_shape=input_shape))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kn7LQVDT5yl",
        "colab_type": "code",
        "outputId": "09c47b9e-26b5-40b2-9fdc-07ec0564db28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        }
      },
      "source": [
        "model.compile(optimizer='adadelta',\n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "model.fit(x=x_train,y=y_train, batch_size=128, epochs=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/10\n",
            "697932/697932 [==============================] - 77s 111us/step - loss: 0.5768 - acc: 0.8114\n",
            "Epoch 2/10\n",
            "  5248/697932 [..............................] - ETA: 1:14 - loss: 0.4567 - acc: 0.8436"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ce7c36546b9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kw1kTd7hVKap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.evaluate(x_test, y_test)\n",
        "\n",
        "# First model\n",
        "# [0.37821523829298365, 0.8649106367619439]\n",
        "\n",
        "# Second model\n",
        "# [0.36202002978185105, 0.8665526164220284]\n",
        "\n",
        "# Third model with 62 in the second conv2d layer\n",
        "# [0.3531124924307382, 0.8706188801870653]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3J5azLJVpKI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image_index = 598\n",
        "plt.imshow(x_test[image_index].reshape(28, 28),cmap='Greys')\n",
        "pred = model.predict(x_test[image_index].reshape(1, 28, 28, 1))\n",
        "print(pred.argmax(),y_test[image_index])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D8uXq8SaZpy",
        "colab_type": "text"
      },
      "source": [
        "### Save model for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5TwZz6Hagm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver()\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver.save(sess, \"character_detection_model_v3\")\n",
        "\n",
        "# Save JSON config to disk\n",
        "json_config = model.to_json()\n",
        "with open('model.json', 'w') as json_file:\n",
        "    json_file.write(json_config)\n",
        "# Save weights to disk\n",
        "model.save_weights('model_weights.h5')\n",
        "\n",
        "# Importing a pre trained model\n",
        "# with tf.Session() as sess:\n",
        "#   new_saver = tf.train.import_meta_graph('model_name.meta')\n",
        "#   new_saver.restore(sess, tf.train.latest_checkpoint('./'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zIoGfjhU0_t",
        "colab_type": "text"
      },
      "source": [
        "### Transforming the input image to the right format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J62AiUttMVOu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image, ImageFilter\n",
        "def imageprepare(argv):\n",
        "    \"\"\"\n",
        "    This function returns the pixel values.\n",
        "    The imput is a png file location.\n",
        "    \"\"\"\n",
        "    im = Image.open(argv).convert('L')\n",
        "    width = float(im.size[0])\n",
        "    height = float(im.size[1])\n",
        "    newImage = Image.new('L', (28, 28), (255))  # creates white canvas of 28x28 pixels\n",
        "\n",
        "    if width > height:  # check which dimension is bigger\n",
        "        # Width is bigger. Width becomes 20 pixels.\n",
        "        nheight = int(round((20.0 / width * height), 0))  # resize height according to ratio width\n",
        "        if (nheight == 0):  # rare case but minimum is 1 pixel\n",
        "            nheight = 1\n",
        "            # resize and sharpen\n",
        "        img = im.resize((20, nheight), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
        "        wtop = int(round(((28 - nheight) / 2), 0))  # calculate horizontal position\n",
        "        newImage.paste(img, (4, wtop))  # paste resized image on white canvas\n",
        "    else:\n",
        "        # Height is bigger. Heigth becomes 20 pixels.\n",
        "        nwidth = int(round((20.0 / height * width), 0))  # resize width according to ratio height\n",
        "        if (nwidth == 0):  # rare case but minimum is 1 pixel\n",
        "            nwidth = 1\n",
        "            # resize and sharpen\n",
        "        img = im.resize((nwidth, 20), Image.ANTIALIAS).filter(ImageFilter.SHARPEN)\n",
        "        wleft = int(round(((28 - nwidth) / 2), 0))  # caculate vertical pozition\n",
        "        newImage.paste(img, (wleft, 4))  # paste resized image on white canvas\n",
        "\n",
        "    # newImage.save(\"sample.png\n",
        "\n",
        "    tv = list(newImage.getdata())  # get pixel values\n",
        "\n",
        "    # normalize pixels to 0 and 1. 0 is pure white, 1 is pure black.\n",
        "    tva = [(255 - x) * 1.0 / 255.0 for x in tv]\n",
        "    print(tva)\n",
        "    return tva"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyVXOtuwK4gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5m46jq-U-BX",
        "colab_type": "text"
      },
      "source": [
        "### Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk-E76guLZBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x=[imageprepare('g.PNG')]\n",
        "\n",
        "newArr=[[0 for d in range(28)] for y in range(28)]\n",
        "k = 0\n",
        "for i in range(28):\n",
        "    for j in range(28):\n",
        "        newArr[i][j]=x[0][k]\n",
        "        k=k+1\n",
        "        \n",
        "newArr = np.array(newArr)\n",
        "plt.imshow(newArr.reshape(28, 28),cmap='Greys')\n",
        "pred = model.predict(newArr.reshape(1, 28, 28, 1))\n",
        "\n",
        "label = pred.argmax()\n",
        "if (label <= 9):\n",
        "  print(\"Digit:\", chr(48+label), \"[\"+str(label)+\"]\")\n",
        "elif (label <= 35):\n",
        "  print(\"Uppercase:\", chr(65+label-10), \"[\"+str(label)+\"]\")\n",
        "elif (label <= 62):\n",
        "  print(\"Lowercase:\", chr(97+label-36), \"[\"+str(label)+\"]\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}